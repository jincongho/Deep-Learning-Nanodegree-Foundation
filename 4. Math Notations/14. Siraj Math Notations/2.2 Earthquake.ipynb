{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  19.246   145.616   131.6   ]\n",
      " [   1.863   127.352    80.    ]\n",
      " [ -20.579  -173.972    20.    ]\n",
      " ..., \n",
      " [  36.9179  140.4262   10.    ]\n",
      " [  -9.0283  118.6639   79.    ]\n",
      " [  37.3973  141.4103   11.94  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6. ,  5.8,  6.2, ...,  5.9,  6.3,  5.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract data from CSV\n",
    "df1=pd.read_csv(\"earthquake_data.csv\")\n",
    "#col1 = df1[['Date','Time','Latitude','Longitude']]\n",
    "col1 = df1[['Latitude','Longitude','Depth']]\n",
    "col2 = df1['Magnitude']\n",
    "#Convert to Numpy array\n",
    "InputX1 = col1.as_matrix()\n",
    "InputY1 = col2.as_matrix()\n",
    "print(InputX1)\n",
    "InputX1.astype(float, copy=False)\n",
    "InputY1.astype(float, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mininum values: [ -77.08  -179.997   -1.1  ]\n",
      "Maximum values: [  86.005  179.998  700.   ]\n"
     ]
    }
   ],
   "source": [
    "#Min-max Normalization\n",
    "X1_min = np.amin(InputX1,0)     \n",
    "X1_max = np.amax(InputX1,0)   \n",
    "print(\"Mininum values:\",X1_min)\n",
    "print(\"Maximum values:\",X1_max)\n",
    "Y1_min = np.amin(InputY1)     \n",
    "Y1_max = np.amax(InputY1) \n",
    "InputX1_norm = (InputX1-X1_min)/(X1_max-X1_min)\n",
    "InputY1_norm = InputY1  #No normalization in output\n",
    "#InputY1_norm = (InputY1-Y1_min)/(Y1_max-Y1_min)\n",
    "\n",
    "#Reshape\n",
    "Xfeatures = 3 #Number of input features\n",
    "Yfeatures = 1 #Number of input features\n",
    "samples = 23000 # Number of samples\n",
    "\n",
    "InputX1_reshape = np.resize(InputX1_norm,(samples,Xfeatures))\n",
    "InputY1_reshape = np.resize(InputY1_norm,(samples,Yfeatures))\n",
    "#print(\"X1 normalized:\",InputX1_reshape)\n",
    "#print(\"Y1 normalized:\",InputY1_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "batch_size = 20000\n",
    "InputX1train = InputX1_reshape[0:batch_size,:]\n",
    "InputY1train = InputY1_reshape[0:batch_size,:]\n",
    "#Validation data\n",
    "v_size = 2500\n",
    "InputX1v = InputX1_reshape[batch_size:batch_size+v_size,:]\n",
    "InputY1v = InputY1_reshape[batch_size:batch_size+v_size,:]\n",
    "#print(InputX1v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Network hyper parametres\n",
    "learning_rate = 0.001\n",
    "training_iterations = 100000\n",
    "display_iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "X = tf.placeholder(tf.float32,shape=(None,Xfeatures))#[batch size, input_features]\n",
    "#Output\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neurons\n",
    "L1 = 3 #Number of neurons in 1st layer\n",
    "L2 = 3 #Number of neurons in 2nd layer\n",
    "L3 = 3 #Number of neurons in 2nd layer\n",
    "#Layer1 weights\n",
    "W_fc1 = tf.Variable(tf.random_uniform([Xfeatures,L1])) # [input_features,Number of neurons]) \n",
    "b_fc1 = tf.Variable(tf.constant(0.1,shape=[L1]))\n",
    "#Layer2 weights\n",
    "W_fc2 = tf.Variable(tf.random_uniform([L1,L2])) # [Number of neurons in preceding layer,Number of neurons]) \n",
    "b_fc2 = tf.Variable(tf.constant(0.1,shape=[L2]))\n",
    "#Layer3 weights\n",
    "W_fc3 = tf.Variable(tf.random_uniform([L2,L3])) # [Number of neurons in preceding layer,Number of neurons]) \n",
    "b_fc3 = tf.Variable(tf.constant(0.1,shape=[L3]))\n",
    "#Output layer weights\n",
    "W_fO= tf.Variable(tf.random_uniform([L3,Yfeatures])) #  [Number of neurons in preceding layer,output_features]) \n",
    "b_fO = tf.Variable(tf.constant(0.1,shape=[Yfeatures]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Layer 1\n",
    "matmul_fc1=tf.matmul(X, W_fc1) + b_fc1\n",
    "h_fc1 = tf.nn.relu(matmul_fc1)   #ReLU activation\n",
    "#Layer 2\n",
    "matmul_fc2=tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "h_fc2 = tf.nn.relu(matmul_fc2)   #ReLU activation\n",
    "#Layer 3\n",
    "matmul_fc3=tf.matmul(h_fc2, W_fc3) + b_fc3\n",
    "h_fc3 = tf.nn.relu(matmul_fc3)   #ReLU activation\n",
    "#Output layer\n",
    "matmul_fc4=tf.matmul(h_fc3, W_fO) + b_fO\n",
    "output_layer = matmul_fc4  #linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loss function\n",
    "mean_square =  tf.reduce_mean(tf.square(Y-output_layer))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(mean_square)\n",
    "#train_step = tf.train.AdagradOptimizer(learning_rate).minimize(mean_square)\n",
    "#train_step = tf.train.MomentumOptimizer(learning_rate,0.01).minimize(mean_square)\n",
    "#train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_square)\n",
    "#Operation to save variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: [4.6381264]\n",
      "Training loss is: [4.5309277] at itertion: 0\n",
      "Validation loss is: [4.717792] at itertion: 0\n",
      "Training loss is: [0.17617485] at itertion: 20000\n",
      "Validation loss is: [0.18727431] at itertion: 20000\n",
      "Training loss is: [0.17617068] at itertion: 40000\n",
      "Validation loss is: [0.18727618] at itertion: 40000\n",
      "Training loss is: [0.17616746] at itertion: 60000\n",
      "Validation loss is: [0.18727437] at itertion: 60000\n",
      "Training loss is: [0.17616335] at itertion: 80000\n",
      "Validation loss is: [0.18727487] at itertion: 80000\n",
      "Model saved in file: /tmp/earthquake_model.ckpt\n",
      "Final training loss: [0.17616388]\n",
      "Final validation loss: [0.18727615]\n"
     ]
    }
   ],
   "source": [
    "#Initialization and session\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Training loss:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    for i in range(training_iterations):\n",
    "        sess.run([train_step],feed_dict={X:InputX1train,Y:InputY1train})\n",
    "        if i%display_iterations ==0:\n",
    "            print(\"Training loss is:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}),\"at itertion:\",i)\n",
    "            print(\"Validation loss is:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}),\"at itertion:\",i)\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, \"/tmp/earthquake_model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    print(\"Final training loss:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    print(\"Final validation loss:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Latitude between -77 to 86:45.71\n",
      "Enter Longitude between -180 to 180:26.5\n",
      "Enter Depth between 0 to 700:97\n",
      "Model restored.\n",
      "output: [array([[ 5.89945841]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "lat = input(\"Enter Latitude between -77 to 86:\")\n",
    "long = input(\"Enter Longitude between -180 to 180:\")\n",
    "depth = input(\"Enter Depth between 0 to 700:\")\n",
    "InputX2 = np.asarray([[lat,long,depth]],dtype=np.float32)\n",
    "InputX2_norm = (InputX2-X1_min)/(X1_max-X1_min)\n",
    "InputX1test = np.resize(InputX2_norm,(1,Xfeatures))\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk for validation.\n",
    "    saver.restore(sess, \"/tmp/earthquake_model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    #print(\"Final validation loss:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "    print(\"output:\",sess.run([output_layer],feed_dict={X:InputX1test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
